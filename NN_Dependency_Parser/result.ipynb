{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paper: Chen, D., & Manning, C. (2014). A fast and accurate dependency parser using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Component - Neural Network Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### conll_reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class DependencyEdge(object):\n",
    "    \"\"\"\n",
    "    Represent a single dependency edge: \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ident, word, pos, head, deprel): \n",
    "        self.id = ident\n",
    "        self.word = word\n",
    "        self.pos = pos\n",
    "        self.head = head\n",
    "        self.deprel = deprel\n",
    "    \n",
    "    def print_conll(self):\n",
    "        return \"{d.id}\\t{d.word}\\t_\\t_\\t{d.pos}\\t_\\t{d.head}\\t{d.deprel}\\t_\\t_\".format(d=self)\n",
    "    \n",
    "\n",
    "def parse_conll_relation(s):\n",
    "    fields = s.split('\\t')\n",
    "    ident_s, word, lemma, upos, pos, feats, head_s, deprel, deps, misc = fields\n",
    "    ident = int(ident_s)\n",
    "    head = int(head_s)\n",
    "    return DependencyEdge(ident, word, pos, head, deprel)\n",
    "\n",
    "\n",
    "class DependencyStructure(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.deprels = {}\n",
    "        self.root = None\n",
    "        self.parent_to_children = defaultdict(list)\n",
    "   \n",
    "    def add_deprel(self, deprel):\n",
    "        self.deprels[deprel.id] = deprel\n",
    "        self.parent_to_children[deprel.head].append(deprel.id)\n",
    "        if deprel.head == 0:\n",
    "            self.root = deprel.id\n",
    "    \n",
    "    def __str__(self):\n",
    "        for k,v in self.deprels.items():\n",
    "            print(v)\n",
    "        \n",
    "    def print_tree(self, parent = None):\n",
    "        if not parent:\n",
    "            return self.print_tree(parent = self.root)\n",
    "       \n",
    "        if self.deprels[parent].head == parent:\n",
    "            return self.deprels[parent].word\n",
    "    \n",
    "        children = [self.print_tree(child) for child in self.parent_to_children[parent]]\n",
    "        child_str = \" \".join(children)\n",
    "        return(\"({} {})\".format(self.deprels[parent].word, child_str))\n",
    "    \n",
    "    def words(self):\n",
    "        return [None]+[x.word for (i,x) in self.deprels.items()]\n",
    "    \n",
    "    def pos(self):\n",
    "        return [None]+[x.pos for (i,x) in self.deprels.items()]\n",
    "    \n",
    "    def print_conll(self):\n",
    "        deprels = [v for (k,v) in  sorted(self.deprels.items())]\n",
    "        return \"\\n\".join(deprel.print_conll() for deprel in deprels)\n",
    "\n",
    "\n",
    "def conll_reader(input_file):\n",
    "    current_deps = DependencyStructure() \n",
    "    while True:\n",
    "        line = input_file.readline().strip()\n",
    "        if not line and current_deps:\n",
    "            yield current_deps\n",
    "            current_deps = DependencyStructure() \n",
    "            line = input_file.readline().strip()\n",
    "            if not line:  \n",
    "                break\n",
    "        current_deps.add_deprel(parse_conll_relation(line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Obtaining the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from conll_reader import conll_reader\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_vocabularies(conll_reader):\n",
    "    word_set = defaultdict(int)\n",
    "    pos_set = set()\n",
    "    for dtree in conll_reader:\n",
    "        for ident, node in dtree.deprels.items():\n",
    "            if node.pos != \"CD\" and node.pos!=\"NNP\":\n",
    "                word_set[node.word.lower()] += 1\n",
    "            pos_set.add(node.pos)\n",
    "\n",
    "    word_set = set(x for x in word_set if word_set[x] > 1)\n",
    "\n",
    "    word_list = [\"<CD>\",\"<NNP>\",\"<UNK>\",\"<ROOT>\",\"<NULL>\"] + list(word_set)\n",
    "    pos_list =  [\"<UNK>\",\"<ROOT>\",\"<NULL>\"] + list(pos_set)\n",
    "\n",
    "    return word_list, pos_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word indices...\n",
      "Writing POS indices...\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/train.conll\",'r') as in_file, open(\"data/words.vocab\",'w') as word_file, open(\"data/pos.vocab\",'w') as pos_file:\n",
    "        word_list, pos_list = get_vocabularies(conll_reader(in_file))\n",
    "        print(\"Writing word indices...\")\n",
    "        for index, word in enumerate(word_list): \n",
    "            word_file.write(\"{}\\t{}\\n\".format(word, index))\n",
    "        print(\"Writing POS indices...\")\n",
    "        for index, pos in enumerate(pos_list): \n",
    "            pos_file.write(\"{}\\t{}\\n\".format(pos, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Extracting Input/Output matrices for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_tranning_data.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll_reader import DependencyStructure, conll_reader\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import sys\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self, sentence = []):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        if sentence: \n",
    "            self.buffer = list(reversed(sentence))\n",
    "        self.deps = set() \n",
    "    \n",
    "    def shift(self):\n",
    "        self.stack.append(self.buffer.pop())\n",
    "\n",
    "    def left_arc(self, label):\n",
    "        self.deps.add( (self.buffer[-1], self.stack.pop(),label) )\n",
    "\n",
    "    def right_arc(self, label):\n",
    "        parent = self.stack.pop()\n",
    "        self.deps.add( (parent, self.buffer.pop(), label) )\n",
    "        self.buffer.append(parent)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{},{},{}\".format(self.stack, self.buffer, self.deps)\n",
    "\n",
    "    \n",
    "def apply_sequence(seq, sentence):\n",
    "    state = State(sentence)\n",
    "    for rel, label in seq:\n",
    "        if rel == \"shift\":\n",
    "            state.shift()\n",
    "        elif rel == \"left_arc\":\n",
    "            state.left_arc(label) \n",
    "        elif rel == \"right_arc\":\n",
    "            state.right_arc(label) \n",
    "         \n",
    "    return state.deps\n",
    "   \n",
    "class RootDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = 0\n",
    "        self.deprel = None    \n",
    "    def __repr__(self):\n",
    "        return \"<ROOT>\"\n",
    "\n",
    "     \n",
    "def get_training_instances(dep_structure):\n",
    "\n",
    "    deprels = dep_structure.deprels\n",
    "    \n",
    "    sorted_nodes = [k for k,v in sorted(deprels.items())]\n",
    "    state = State(sorted_nodes)\n",
    "    state.stack.append(0)\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for ident,node in deprels.items():\n",
    "        childcount[node.head] += 1\n",
    " \n",
    "    seq = []\n",
    "    while state.buffer: \n",
    "        if not state.stack:\n",
    "            seq.append((copy.deepcopy(state),(\"shift\",None)))\n",
    "            state.shift()\n",
    "            continue\n",
    "        if state.stack[-1] == 0:\n",
    "            stackword = RootDummy() \n",
    "        else:\n",
    "            stackword = deprels[state.stack[-1]]\n",
    "        bufferword = deprels[state.buffer[-1]]\n",
    "        if stackword.head == bufferword.id:\n",
    "            childcount[bufferword.id]-=1\n",
    "            seq.append((copy.deepcopy(state),(\"left_arc\",stackword.deprel)))\n",
    "            state.left_arc(stackword.deprel)\n",
    "        elif bufferword.head == stackword.id and childcount[bufferword.id] == 0:\n",
    "            childcount[stackword.id]-=1\n",
    "            seq.append((copy.deepcopy(state),(\"right_arc\",bufferword.deprel)))\n",
    "            state.right_arc(bufferword.deprel)\n",
    "        else: \n",
    "            seq.append((copy.deepcopy(state),(\"shift\",None)))\n",
    "            state.shift()\n",
    "    return seq   \n",
    "\n",
    "\n",
    "dep_relations = ['tmod', 'vmod', 'csubjpass', 'rcmod', 'ccomp', 'poss', 'parataxis', 'appos', 'dep', 'iobj', 'pobj', 'mwe', 'quantmod', 'acomp', 'number', 'csubj', 'root', 'auxpass', 'prep', 'mark', 'expl', 'cc', 'npadvmod', 'prt', 'nsubj', 'advmod', 'conj', 'advcl', 'punct', 'aux', 'pcomp', 'discourse', 'nsubjpass', 'predet', 'cop', 'possessive', 'nn', 'xcomp', 'preconj', 'num', 'amod', 'dobj', 'neg','dt','det']\n",
    "\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "       \n",
    "    def __init__(self, word_vocab_file, pos_vocab_file):\n",
    "        self.word_vocab = self.read_vocab(word_vocab_file)        \n",
    "        self.pos_vocab = self.read_vocab(pos_vocab_file)        \n",
    "        self.output_labels = self.make_output_labels()\n",
    "\n",
    "    def make_output_labels(self):\n",
    "        labels = []\n",
    "        labels.append(('shift',None))\n",
    "    \n",
    "        for rel in dep_relations:\n",
    "            labels.append((\"left_arc\",rel))\n",
    "            labels.append((\"right_arc\",rel))\n",
    "        return dict((label, index) for (index,label) in enumerate(labels))\n",
    "\n",
    "    def read_vocab(self,vocab_file):\n",
    "        vocab = {}\n",
    "        for line in vocab_file: \n",
    "            word, index_s = line.strip().split()\n",
    "            index = int(index_s)\n",
    "            vocab[word] = index\n",
    "        return vocab     \n",
    "\n",
    "    def get_input_representation(self, words, pos, state):\n",
    "        # TODO: Write this method for Part 2\n",
    "        # return a single vector of 6 \n",
    "        # the idea\n",
    "        # 1: when state.stack[position] is 0, the current word is considered as\"<Root>\"\n",
    "        # 2: \"<NULL>\": padding context window\n",
    "        # 3: if pos[state.stack[position]] == \"CD\", consider the current word as \"<CD>\"\n",
    "        # 4: if pos[state.stack[position]] == \"NNP\", consider the current word as \"<NNP>\"\n",
    "        # 5: otherwise: consider the current word as \"<UNK>\"\n",
    "        \n",
    "        # result list\n",
    "        input_list = []\n",
    "        length_stack = len(state.stack)\n",
    "        position = -1\n",
    "        while position >= -3:\n",
    "            if length_stack > 0:\n",
    "                word_pos = state.stack[position]\n",
    "                if word_pos == 0:\n",
    "                    input_list.append(self.word_vocab[\"<ROOT>\"])\n",
    "                elif pos[word_pos] == \"CD\":\n",
    "                    input_list.append(self.word_vocab[\"<CD>\"])\n",
    "                elif pos[word_pos] == \"NNP\":\n",
    "                    input_list.append(self.word_vocab[\"<NNP>\"])\n",
    "                else:  \n",
    "                    if words[word_pos].lower() in self.word_vocab:\n",
    "                        input_list.append(self.word_vocab[words[word_pos].lower()])\n",
    "                    else:\n",
    "                        input_list.append(self.word_vocab[\"<UNK>\"])\n",
    "            else:\n",
    "                input_list.append(self.word_vocab[\"<NULL>\"])\n",
    "            length_stack = length_stack - 1\n",
    "            position = position - 1\n",
    "         \n",
    "        \n",
    "        length_buffer = len(state.buffer)\n",
    "        position = -1\n",
    "        while position >= -3:\n",
    "            if length_buffer > 0:\n",
    "                word_pos = state.buffer[position]\n",
    "                if word_pos == 0:\n",
    "                    input_list.append(self.word_vocab[\"<ROOT>\"])\n",
    "                elif pos[word_pos] == \"CD\":\n",
    "                    input_list.append(self.word_vocab[\"<CD>\"])\n",
    "                elif pos[word_pos] == \"NNP\":\n",
    "                    input_list.append(self.word_vocab[\"<NNP>\"])\n",
    "                else:\n",
    "                    if words[word_pos].lower() in self.word_vocab:\n",
    "                        input_list.append(self.word_vocab[words[word_pos].lower()])\n",
    "                    else:\n",
    "                        input_list.append(self.word_vocab[\"<UNK>\"])\n",
    "                        \n",
    "            else:\n",
    "                input_list.append(self.word_vocab[\"<NULL>\"])   \n",
    "            length_buffer = length_buffer - 1\n",
    "            position = position - 1\n",
    "\n",
    "        return np.asarray(input_list, dtype=np.int)\n",
    "        \n",
    "\n",
    "    def get_output_representation(self, output_pair):  \n",
    "        # TODO: Write this method for Part 2  \n",
    "        return keras.utils.to_categorical(self.output_labels[output_pair], num_classes=len(self.output_labels), dtype=int)\n",
    "\n",
    "     \n",
    "def get_training_matrices(extractor, in_file):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    count = 0 \n",
    "    for dtree in conll_reader(in_file): \n",
    "        words = dtree.words()\n",
    "        pos = dtree.pos()\n",
    "\n",
    "        for state, output_pair in get_training_instances(dtree):\n",
    "            inputs.append(extractor.get_input_representation(words, pos, state))\n",
    "            outputs.append(extractor.get_output_representation(output_pair))\n",
    "        if count%100 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "        count += 1\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    return np.vstack(inputs),np.vstack(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing get_training_matrices()  for  training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction... (each . represents 100 sentences)\n",
      "...............................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Writing output...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "WORD_VOCAB_FILE = 'data/words.vocab'\n",
    "POS_VOCAB_FILE = 'data/pos.vocab'\n",
    "\n",
    "argv1 = \"data/train.conll\"\n",
    "argv2 = \"data/input_train.npy\"\n",
    "argv3 = \"data/target_train.npy\"\n",
    "\n",
    "try:\n",
    "    word_vocab_f = open(WORD_VOCAB_FILE,'r')\n",
    "    pos_vocab_f = open(POS_VOCAB_FILE,'r') \n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find vocabulary files {} and {}\".format(WORD_VOCAB_FILE, POS_VOCAB_FILE))\n",
    "    sys.exit(1) \n",
    "\n",
    "\n",
    "with open(argv1,'r') as in_file:   \n",
    "\n",
    "    extractor = FeatureExtractor(word_vocab_f, pos_vocab_f)\n",
    "    print(\"Starting feature extraction... (each . represents 100 sentences)\")\n",
    "    inputs, outputs = get_training_matrices(extractor,in_file)\n",
    "    print(\"Writing output...\")\n",
    "    np.save(argv2, inputs)\n",
    "    np.save(argv3, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing get_training_matrices()  for development data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction... (each . represents 100 sentences)\n",
      "...................................................\n",
      "Writing output...\n"
     ]
    }
   ],
   "source": [
    "WORD_VOCAB_FILE = 'data/words.vocab'\n",
    "POS_VOCAB_FILE = 'data/pos.vocab'\n",
    "\n",
    "argv1 = \"data/dev.conll\"\n",
    "argv2 = \"data/input_dev.npy\"\n",
    "argv3 = \"data/target_dev.npy\"\n",
    "\n",
    "try:\n",
    "    word_vocab_f = open(WORD_VOCAB_FILE,'r')\n",
    "    pos_vocab_f = open(POS_VOCAB_FILE,'r') \n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find vocabulary files {} and {}\".format(WORD_VOCAB_FILE, POS_VOCAB_FILE))\n",
    "    sys.exit(1) \n",
    "\n",
    "\n",
    "with open(argv1,'r') as in_file:   \n",
    "\n",
    "    extractor = FeatureExtractor(word_vocab_f, pos_vocab_f)\n",
    "    print(\"Starting feature extraction... (each . represents 100 sentences)\")\n",
    "    inputs, outputs = get_training_matrices(extractor,in_file)\n",
    "    print(\"Writing output...\")\n",
    "    np.save(argv2, inputs)\n",
    "    np.save(argv3, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Designing and Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_training_data import FeatureExtractor\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Flatten, Embedding, Dense\n",
    "# add by Shusen Xu\n",
    "from keras.layers import Activation\n",
    "\n",
    "def build_model(word_types, pos_types, outputs):\n",
    "    # TODO: Write this function for part 3\n",
    "    model = Sequential()\n",
    "    # add Embedding layer\n",
    "    model.add(Embedding(input_dim=word_types, output_dim=32, input_length=6))\n",
    "    # flatten \n",
    "    model.add(Flatten())\n",
    "    # add hidden layers\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    \n",
    "    # add outputlayers\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    model.compile(keras.optimizers.Adam(lr=0.01), loss=\"categorical_crossentropy\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test for trainning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model.\n",
      "Done loading data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1899519/1899519 [==============================] - 80s 42us/step - loss: 0.4914\n",
      "Epoch 2/5\n",
      "1899519/1899519 [==============================] - 80s 42us/step - loss: 0.4147\n",
      "Epoch 3/5\n",
      "1899519/1899519 [==============================] - 82s 43us/step - loss: 0.3997\n",
      "Epoch 4/5\n",
      "1899519/1899519 [==============================] - 83s 44us/step - loss: 0.3915\n",
      "Epoch 5/5\n",
      "1899519/1899519 [==============================] - 83s 44us/step - loss: 0.3850\n"
     ]
    }
   ],
   "source": [
    "WORD_VOCAB_FILE = 'data/words.vocab'\n",
    "POS_VOCAB_FILE = 'data/pos.vocab'\n",
    "argv1 = \"data/input_train.npy\"\n",
    "argv2 = \"data/target_train.npy\"\n",
    "argv3 = \"data/model.h5\"\n",
    "\n",
    "\n",
    "try:\n",
    "    word_vocab_f = open(WORD_VOCAB_FILE,'r')\n",
    "    pos_vocab_f = open(POS_VOCAB_FILE,'r') \n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find vocabulary files {} and {}\".format(WORD_VOCAB_FILE, POS_VOCAB_FILE))\n",
    "    sys.exit(1) \n",
    "\n",
    "extractor = FeatureExtractor(word_vocab_f, pos_vocab_f)\n",
    "print(\"Compiling model.\")\n",
    "model = build_model(len(extractor.word_vocab), len(extractor.pos_vocab), len(extractor.output_labels))\n",
    "inputs = np.load(argv1)\n",
    "outputs = np.load(argv2)\n",
    "print(\"Done loading data.\")\n",
    "\n",
    "# Now train the model\n",
    "model.fit(inputs, outputs, epochs=5, batch_size=100)\n",
    "\n",
    "model.save(argv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Greedy Parsing Algorithm - Building and Evaluating the Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll_reader import DependencyStructure, DependencyEdge, conll_reader\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from extract_training_data import FeatureExtractor, State\n",
    "\n",
    "class Parser(object): \n",
    "\n",
    "    def __init__(self, extractor, modelfile):\n",
    "        self.model = keras.models.load_model(modelfile)\n",
    "        self.extractor = extractor\n",
    "        \n",
    "        # The following dictionary from indices to output actions will be useful\n",
    "        self.output_labels = dict([(index, action) for (action, index) in extractor.output_labels.items()])\n",
    "\n",
    "    def parse_sentence(self, words, pos):\n",
    "        state = State(range(1,len(words)))\n",
    "        state.stack.append(0)    \n",
    "        \n",
    "        while state.buffer: \n",
    "            # pass\n",
    "            # TODO: Write the body of this loop for part 4 \n",
    "            # step 1: \n",
    "            features = self.extractor.get_input_representation(words, pos, state)\n",
    "            possible_actions = self.model.predict(features.reshape([1,6]))\n",
    "            possible_actions = possible_actions.reshape(91)\n",
    "            # step 2: select the highest scoring permitted transition\n",
    "           \n",
    "            # create a possible action indices list sorted by their possibility(largest one comes first)\n",
    "            # sorted_actions_indices = np.flipud(np.argsort(possible_actions))\n",
    "            sorted_actions_indices = np.flipud(np.argsort(possible_actions))\n",
    "            \n",
    "            # going through and find the highest scoring permitted trasition\n",
    "            for i in sorted_actions_indices:\n",
    "                flag = False\n",
    "                # check the current transition whether permitted or not\n",
    "                if self.output_labels[i][0] == \"shift\":\n",
    "                    if state.stack and len(state.buffer) == 1:\n",
    "                        flag = False\n",
    "                    else:\n",
    "                        flag = True    \n",
    "                        \n",
    "                elif self.output_labels[i][0] == \"left_arc\":\n",
    "                    if not state.stack:\n",
    "                        flag = False\n",
    "                    elif state.stack[-1] == 0:\n",
    "                        flag = False\n",
    "                    else:\n",
    "                        flag = True\n",
    "           \n",
    "                elif self.output_labels[i][0] == \"right_arc\":\n",
    "                    if not state.stack:\n",
    "                        flag = False\n",
    "                    else: flag = True\n",
    "                \n",
    "                # when flag == True, it states that the cuurent transition is permitted\n",
    "                if flag == True:\n",
    "                    transition = self.output_labels[i]\n",
    "                    # update the state accordingly\n",
    "                    if transition[0] == \"shift\":\n",
    "                        state.shift()\n",
    "                    elif transition[0] == \"left_arc\":\n",
    "                        state.left_arc(transition[1])\n",
    "                    elif transition[0] == \"right_arc\":\n",
    "                        state.right_arc(transition[1])   \n",
    "                    break\n",
    "    \n",
    "        result = DependencyStructure()\n",
    "        for p,c,r in state.deps: \n",
    "            result.add_deprel(DependencyEdge(c,words[c],pos[c],p, r))\n",
    "        return result \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test for decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "WORD_VOCAB_FILE = 'data/words.vocab'\n",
    "POS_VOCAB_FILE = 'data/pos.vocab'\n",
    "\n",
    "argv1 = \"data/model.h5\"\n",
    "argv2 = \"data/dev.conll\"\n",
    "\n",
    "\n",
    "try:\n",
    "    word_vocab_f = open(WORD_VOCAB_FILE,'r')\n",
    "    pos_vocab_f = open(POS_VOCAB_FILE,'r') \n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find vocabulary files {} and {}\".format(WORD_VOCAB_FILE, POS_VOCAB_FILE))\n",
    "    sys.exit(1) \n",
    "\n",
    "extractor = FeatureExtractor(word_vocab_f, pos_vocab_f)\n",
    "parser = Parser(extractor, argv1)\n",
    "\n",
    "with open(argv2,'r') as in_file: \n",
    "    for dtree in conll_reader(in_file):\n",
    "        words = dtree.words()\n",
    "        pos = dtree.pos()\n",
    "        deps = parser.parse_sentence(words, pos)\n",
    "        #print(deps.print_conll())\n",
    "        #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import Parser\n",
    "from extract_training_data import FeatureExtractor\n",
    "from conll_reader import conll_reader\n",
    "import sys\n",
    "\n",
    "\n",
    "def compare_parser(target, predict):\n",
    "    target_unlabeled = set((d.id,d.head) for d in target.deprels.values())\n",
    "    target_labeled = set((d.id,d.head,d.deprel) for d in target.deprels.values())\n",
    "    predict_unlabeled = set((d.id,d.head) for d in predict.deprels.values())\n",
    "    predict_labeled = set((d.id,d.head,d.deprel) for d in predict.deprels.values())\n",
    "\n",
    "    labeled_correct = len(predict_labeled.intersection(target_labeled))\n",
    "    unlabeled_correct = len(predict_unlabeled.intersection(target_unlabeled))\n",
    "    num_words = len(predict_labeled)\n",
    "    return labeled_correct, unlabeled_correct, num_words \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test for evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating. (Each . represents 100 test dependency trees)\n",
      "..................................................\n",
      "5039 sentence.\n",
      "\n",
      "Micro Avg. Labeled Attachment Score: 0.695314378580964\n",
      "Micro Avg. Unlabeled Attachment Score: 0.7512997189124648\n",
      "\n",
      "Macro Avg. Labeled Attachment Score: 0.7049957030377082\n",
      "Macro Avg. Unlabeled Attachment Score: 0.7612958100753767\n"
     ]
    }
   ],
   "source": [
    "WORD_VOCAB_FILE = 'data/words.vocab'\n",
    "POS_VOCAB_FILE = 'data/pos.vocab'\n",
    "\n",
    "argv1 = \"data/model.h5\"\n",
    "argv2 = \"data/dev.conll\"\n",
    "\n",
    "\n",
    "try:\n",
    "    word_vocab_f = open(WORD_VOCAB_FILE,'r')\n",
    "    pos_vocab_f = open(POS_VOCAB_FILE,'r') \n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find vocabulary files {} and {}\".format(WORD_VOCAB_FILE, POS_VOCAB_FILE))\n",
    "    sys.exit(1) \n",
    "\n",
    "extractor = FeatureExtractor(word_vocab_f, pos_vocab_f)\n",
    "parser = Parser(extractor, argv1)\n",
    "\n",
    "\n",
    "total_labeled_correct = 0\n",
    "total_unlabeled_correct = 0\n",
    "total_words = 0\n",
    "\n",
    "las_list = []\n",
    "uas_list = []    \n",
    "\n",
    "count = 0 \n",
    "\n",
    "\n",
    "with open(argv2,'r') as in_file: \n",
    "    print(\"Evaluating. (Each . represents 100 test dependency trees)\")\n",
    "    for dtree in conll_reader(in_file):\n",
    "        words = dtree.words()\n",
    "        pos = dtree.pos()\n",
    "        predict = parser.parse_sentence(words, pos)\n",
    "        labeled_correct, unlabeled_correct, num_words = compare_parser(dtree, predict)\n",
    "        las_s = labeled_correct / float(num_words)\n",
    "        uas_s = unlabeled_correct / float(num_words)\n",
    "        las_list.append(las_s)\n",
    "        uas_list.append(uas_s)\n",
    "        total_labeled_correct += labeled_correct\n",
    "        total_unlabeled_correct += unlabeled_correct\n",
    "        total_words += num_words\n",
    "        count +=1 \n",
    "        if count % 100 == 0:\n",
    "            print(\".\",end=\"\")\n",
    "            sys.stdout.flush()\n",
    "print()\n",
    "\n",
    "las_micro = total_labeled_correct / float(total_words)\n",
    "uas_micro = total_unlabeled_correct / float(total_words)\n",
    "\n",
    "las_macro = sum(las_list) / len(las_list)\n",
    "uas_macro = sum(uas_list) / len(uas_list)\n",
    "\n",
    "print(\"{} sentence.\\n\".format(len(las_list)))\n",
    "print(\"Micro Avg. Labeled Attachment Score: {}\".format(las_micro))\n",
    "print(\"Micro Avg. Unlabeled Attachment Score: {}\\n\".format(uas_micro))\n",
    "print(\"Macro Avg. Labeled Attachment Score: {}\".format(las_macro))\n",
    "print(\"Macro Avg. Unlabeled Attachment Score: {}\".format(uas_macro))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
