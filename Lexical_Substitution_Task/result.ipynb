{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('interruption.n.02.break'),\n",
       " Lemma('break.n.02.break'),\n",
       " Lemma('fault.n.04.break'),\n",
       " Lemma('rupture.n.02.break'),\n",
       " Lemma('respite.n.02.break'),\n",
       " Lemma('breakage.n.03.break'),\n",
       " Lemma('pause.n.01.break'),\n",
       " Lemma('fracture.n.01.break'),\n",
       " Lemma('break.n.09.break'),\n",
       " Lemma('break.n.10.break'),\n",
       " Lemma('break.n.11.break'),\n",
       " Lemma('break.n.12.break'),\n",
       " Lemma('break.n.13.break'),\n",
       " Lemma('break.n.14.break'),\n",
       " Lemma('open_frame.n.01.break'),\n",
       " Lemma('break.n.16.break')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wn.lemmas('break', pos='n') # Retrieve all lexemes for the noun 'break'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = wn.lemmas('break', pos='n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = l1.synset() # get the synset for the first lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('interruption.n.02')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('interruption.n.02.interruption'), Lemma('interruption.n.02.break')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.lemmas() # Get all lexemes in that synset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interruption'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " s1.lemmas()[0].name() # Get the word of the first lexeme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some abrupt occurrence that interrupts an ongoing activity'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the telephone is an annoying interruption',\n",
       " 'there was a break in the action when a player was hurt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('happening.n.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dislocation.n.01'),\n",
       " Synset('eclipse.n.01'),\n",
       " Synset('punctuation.n.01'),\n",
       " Synset('suspension.n.04')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim is a vector space modeling package for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "v1 = model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33398882"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('computer','calculator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26003766"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('computer','toaster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12194334"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('computer','dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09933449"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('computer','run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to compute similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(v1,v2):\n",
    "    return np.dot(v1,v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33398882"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(model.wv['computer'],model.wv['calculator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexsub_xml.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__==\"__main__\":\\n\\n    for context in read_lexsub_xml(sys.argv[1]):\\n        print(context)\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import tostring\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "class Context(object):\n",
    "    \"\"\"\n",
    "    Represent a single input word with context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cid, word_form, lemma, pos, left_context, right_context): \n",
    "        self.cid = cid\n",
    "        self.word_form = word_form\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "        self.left_context = left_context\n",
    "        self.right_context = right_context\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Context_{cid}/{lemma}.{pos} {left} *{word}* {right}>\".format(cid=self.cid, lemma = self.lemma, pos = self.pos, left = \" \".join(self.left_context), word=self.word_form, right=\" \".join(self.right_context))\n",
    "\n",
    "class LexsubData(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.total_count =  1\n",
    "        pass\n",
    "\n",
    "    def process_context(self, context_s):\n",
    "        head_re = re.compile(\"<head>(.*)</head>\")\n",
    "        match =  head_re.search(context_s)\n",
    "        target = match.groups(1)[0]\n",
    "        context_left = context_s[:match.start()]\n",
    "        context_right = context_s[match.end():]\n",
    "        return target, context_left.split(), context_right.split()\n",
    "\n",
    "    def parse_lexelt(self, lexelt):\n",
    "        lex_item = lexelt.get('item')\n",
    "        parts = lex_item.split('.')\n",
    "        if len(parts) == 3:\n",
    "            lemma, pos = parts[0], parts[2]\n",
    "        else: \n",
    "            lemma, pos = parts[0], parts[1]\n",
    "\n",
    "        for instance in lexelt:\n",
    "            assert instance.tag==\"instance\"\n",
    "            context = instance.find(\"context\")                     \n",
    "            context_s = \"\".join([str(context.text)] + [codecs.decode(ET.tostring(e),\"UTF-8\") for e in context])\n",
    "            word_form, left_context, right_context = self.process_context(context_s)\n",
    "            yield Context(self.total_count, word_form, lemma, pos, left_context, right_context)\n",
    "            self.total_count += 1\n",
    "\n",
    "    def parse_et(self,et):\n",
    "       assert et.tag == \"corpus\"\n",
    "       for lexelt in et: \n",
    "            assert lexelt.tag == \"lexelt\"\n",
    "            for annotation in self.parse_lexelt(lexelt):\n",
    "                yield annotation\n",
    "\n",
    "\n",
    "def read_lexsub_xml(*sources):\n",
    "    \"\"\"\n",
    "    Parse the lexical substitution data and return an iterator over Context instances.\n",
    "    \"\"\"\n",
    "    lexsub_data = LexsubData()\n",
    "    for source_f in sources:\n",
    "        et = ET.parse(source_f)\n",
    "        for annotation in lexsub_data.parse_et(et.getroot()):\n",
    "            yield annotation\n",
    "'''\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    for context in read_lexsub_xml(sys.argv[1]):\n",
    "        print(context)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Candidate Synonyms from WordNet (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexsub_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "from lexsub_xml import read_lexsub_xml\n",
    "\n",
    "# suggested imports \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__==\"__main__\":\\n\\n    # At submission time, this program should run your best predictor (part 6).\\n\\n    #W2VMODEL_FILENAME = \\'GoogleNews-vectors-negative300.bin.gz\\'\\n    #predictor = Word2VecSubst(W2VMODEL_FILENAME)\\n\\n    for context in read_lexsub_xml(sys.argv[1]):\\n        #print(context)  # useful for debugging\\n        prediction = smurf_predictor(context) \\n        print(\"{}.{} {} :: {}\".format(context.lemma, context.pos, context.cid, prediction))\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Participate in the 4705 lexical substitution competition (optional): NO\n",
    "# Alias: [please invent some name]\n",
    "\n",
    "def tokenize(s):\n",
    "    s = \"\".join(\" \" if x in string.punctuation else x for x in s.lower())    \n",
    "    return s.split() \n",
    "\n",
    "def get_candidates(lemma, pos):\n",
    "    # Part 1\n",
    "    # return value a list\n",
    "    possible_synonyms = []\n",
    "    \n",
    "    temp_set = set()\n",
    "    for l in wn.lemmas(lemma, pos):\n",
    "                for s_l in l.synset().lemmas():\n",
    "                    if(s_l.name() != lemma):\n",
    "                        temp_set.add(s_l.name())\n",
    "                    \n",
    "    possible_synonyms = list(temp_set)    \n",
    "    \n",
    "    return possible_synonyms\n",
    "\n",
    "def smurf_predictor(context):\n",
    "    \"\"\"\n",
    "    Just suggest 'smurf' as a substitute for all words.\n",
    "    \"\"\"\n",
    "    return 'smurf'\n",
    "    \n",
    "'''\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # At submission time, this program should run your best predictor (part 6).\n",
    "\n",
    "    #W2VMODEL_FILENAME = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "    #predictor = Word2VecSubst(W2VMODEL_FILENAME)\n",
    "\n",
    "    for context in read_lexsub_xml(sys.argv[1]):\n",
    "        #print(context)  # useful for debugging\n",
    "        prediction = smurf_predictor(context) \n",
    "        print(\"{}.{} {} :: {}\".format(context.lemma, context.pos, context.cid, prediction))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get started testing\n",
    "argv1 = \"lexsub_trial.xml\"\n",
    "def testSmurf(argv1):\n",
    "    for context in read_lexsub_xml(argv1):\n",
    "        #print(context)  # useful for debugging\n",
    "        prediction = smurf_predictor(context) \n",
    "        print(\"{}.{} {} :: {}\".format(context.lemma, context.pos, context.cid, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boring',\n",
       " 'deadening',\n",
       " 'dense',\n",
       " 'dim',\n",
       " 'dull',\n",
       " 'dumb',\n",
       " 'ho-hum',\n",
       " 'irksome',\n",
       " 'obtuse',\n",
       " 'sluggish',\n",
       " 'tedious',\n",
       " 'tiresome',\n",
       " 'wearisome'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test part 1\n",
    "get_candidates('slow','a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: WordNet Frequency Baseline (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def wn_frequency_predictor(contex):\n",
    "    # replace for part 2\n",
    "    # dict that count the occurency\n",
    "    counter = defaultdict(int)\n",
    "    lemma = context.lemma\n",
    "    pos = context.pos\n",
    "    \n",
    "    for l in wn.lemmas(lemma, pos):\n",
    "                for s_l in l.synset().lemmas():\n",
    "                    if(s_l.name() != lemma):\n",
    "                        \n",
    "                        counter[s_l.name()] += 1\n",
    "    \n",
    "    # find the word with highest frequency\n",
    "    max_word = \" \"\n",
    "    max_frequency = 0\n",
    "    for word, frequency in counter.items():\n",
    "        if(frequency > max_frequency):\n",
    "            max_word = word\n",
    "            max_frequency = frequency\n",
    "        \n",
    "    return max_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dull'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_frequency_predictor('slow','a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Simple Lesk Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_simple_lesk_predictor(context):\n",
    "    #replace for part 3  \n",
    "    largest_overlap_size = 0\n",
    "    synset = None\n",
    "    for l in wn.lemmas(lemma, pos):\n",
    "        for s in l.synset():\n",
    "            set_temp1 = computer_overlap(context, synset)\n",
    "            for h in s1.hypernyms():\n",
    "                set_temp2 = compute_overlap(context, h)\n",
    "                set_temp1.union(set_temp2)\n",
    "            if(len(set_temp1) > largest_overlap_size):\n",
    "                largest_overlap_size = len(set_temp1)\n",
    "                synset = s\n",
    "     \n",
    "    counter = defaultdict(int)\n",
    "    max\n",
    "    \n",
    "    for l in s.lemmas():\n",
    "        if(s_l.name() != lemma):\n",
    "            counter[s_l.name()] +=1\n",
    "    \n",
    "    # find the word with highest frequency\n",
    "    max_word = \" \"\n",
    "    max_frequency = 0\n",
    "    for word, frequency in counter.items():\n",
    "        if(frequency > max_frequency):\n",
    "            max_word = word\n",
    "            max_frequency = frequency\n",
    "            \n",
    "    return max_word\n",
    "\n",
    "def compute_overlap(context, synset):\n",
    "    # the definition\n",
    "    overlap = set()\n",
    "    definition = tokenize(synset.definition)\n",
    "    examples = tokenize(synset.examples())\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    for word in definition:\n",
    "        if(word in stop_words):\n",
    "            continue\n",
    "        if(word in context.left_context or word in context.right_context):\n",
    "            overlap.add(word)\n",
    "            \n",
    "    for example in examples:\n",
    "        for word in example:\n",
    "            if(word in stop_words):\n",
    "                continue\n",
    "            if(word in context.left_context or word in context.right_context):\n",
    "                overlap.add(word)   \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Most Similar Synonym\n",
    "### Part 5: Context and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSubst(object):\n",
    "        \n",
    "    def __init__(self, filename):\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)    \n",
    "\n",
    "    def predict_nearest(self, context):\n",
    "        # replace for part 4\n",
    "        # same as part 1 \n",
    "        possible_synonyms = get_candidates(context.lemma, context.pos)\n",
    "        \n",
    "        highest_similarity = 0.0 \n",
    "        highest_synonym = \"\"\n",
    "        for synonym in possible_synonyms:\n",
    "            # ignoring the vocab that not in the model\n",
    "            if(synonym not in self.model.vocab):\n",
    "                continue\n",
    "            temp_similarity = self.model.similarity(lemma, synonym)\n",
    "            if(temp_similarity > highest_similarity):\n",
    "                highest_similarity = temp_similarity\n",
    "                highest_synonym = synonym\n",
    "        \n",
    "        return highest_synonym\n",
    "    \n",
    "    \n",
    "    def predict_nearest_with_context(self, context): \n",
    "        # replace for part 5\n",
    "        stop_words = stopwords.words('english')\n",
    "        vector_target = self.model.wv[context.lemma]\n",
    "        vector_sentence = np.zeros(vector_target.shape)\n",
    "        possible_synonyms = get_candidates(context.lemma, context.pos)\n",
    "        \n",
    "        # possible \n",
    "        # remove the stop words within +-5 window and add the left data\n",
    "        # build sentence vector within the window\n",
    "        windows_words = []\n",
    "        i = len(context.left_context)-1\n",
    "        j = 0\n",
    "        while(i>=0 and j<5):\n",
    "            if(context.left_context[i] not in stop_words):\n",
    "                windows_words.append(context.left_context[i])\n",
    "            i -= 1\n",
    "            j += 1\n",
    "        \n",
    "        k = 0\n",
    "        while(k< len(context.right_context) and k<5):\n",
    "            if(context.right_context[k] not in stop_words):\n",
    "                windows_words.append(context.right_context[k])\n",
    "            k += 1\n",
    "            \n",
    "        # compute the window words vector\n",
    "        for word in windows_word:\n",
    "            if(word not in self.model.vocab):\n",
    "                continue\n",
    "            vector_sentence += self.model.wv[word]\n",
    "        \n",
    "        # computer the synonym that has highest similarity with the sentence vector\n",
    "        highest_similarity = 0.0 \n",
    "        highest_synonym = \"\"\n",
    "        for synonym in possible_synonyms:\n",
    "            # ignoring the vocab that not in the model\n",
    "            if(synonym not in self.model.vocab):\n",
    "                continue\n",
    "            temp_similarity = self.model.similarity(context.lemma, synonym)\n",
    "            if(temp_similarity > highest_similarity):\n",
    "                highest_similarity = temp_similarity\n",
    "                highest_synonym = synonym\n",
    "        \n",
    "        return highest_synonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6  Other ideas? (and competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
